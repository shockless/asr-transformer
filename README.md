# ASR-Transformer

## Technologies used
- Torch 2.0
- Torchaudio
- Spectrogramms
- Transformer

## Citations

```

@INPROCEEDINGS{8462506,
  author={Dong, Linhao and Xu, Shuang and Xu, Bo},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition}, 
  year={2018},
  volume={},
  number={},
  pages={5884-5888},
  keywords={Hidden Markov models;Encoding;Training;Decoding;Speech recognition;Time-frequency analysis;Spectrogram;Speech Recognition;Sequence-to-Sequence;Attention;Transformer},
  doi={10.1109/ICASSP.2018.8462506}}


@misc{winata2020lightweight,
      title={Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank Transformer}, 
      author={Genta Indra Winata and Samuel Cahyawijaya and Zhaojiang Lin and Zihan Liu and Pascale Fung},
      year={2020},
      eprint={1910.13923},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chorowski2015attentionbased,
      title={Attention-Based Models for Speech Recognition}, 
      author={Jan Chorowski and Dzmitry Bahdanau and Dmitriy Serdyuk and Kyunghyun Cho and Yoshua Bengio},
      year={2015},
      eprint={1506.07503},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


### TODO:
- Train on big corpus and evaluate metrics
- Make beamsearch instead of greedy-search
